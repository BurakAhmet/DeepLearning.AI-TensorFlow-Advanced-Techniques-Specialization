{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6S2HVAkSt0p"
      },
      "source": [
        "# Week 2 Assignment: CIFAR-10 Autoencoder\n",
        "\n",
        "For this week, you will create a convolutional autoencoder for the [CIFAR10](https://www.tensorflow.org/datasets/catalog/cifar10) dataset. You are free to choose the architecture of your autoencoder provided that the output image has the same dimensions as the input image.\n",
        "\n",
        "After training, your model should meet loss and accuracy requirements when evaluated with the test dataset. You will then download the model and upload it in the classroom for grading.\n",
        "\n",
        "Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r4iPr2jyisR"
      },
      "source": [
        "***Important:*** *This colab notebook has read-only access so you won't be able to save your changes. If you want to save your work periodically, please click `File -> Save a Copy in Drive` to create a copy in your account, then work from there.*  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1mzy2J8_nc1"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3EXwoz-KHtWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57dd201e-3c23-4813-890d-74cf5ddbd908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Gs6Lyc_pd0"
      },
      "source": [
        "## Load and prepare the dataset\n",
        "\n",
        "The [CIFAR 10](https://www.tensorflow.org/datasets/catalog/cifar10) dataset already has train and test splits and you can use those in this exercise. Here are the general steps:\n",
        "\n",
        "* Load the train/test split from TFDS. Set `as_supervised` to `True` so it will be convenient to use the preprocessing function we provided.\n",
        "* Normalize the pixel values to the range [0,1], then return `image, image` pairs for training instead of `image, label`. This is because you will check if the output image is successfully regenerated after going through your autoencoder.\n",
        "* Shuffle and batch the train set. Batch the test set (no need to shuffle).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "t9F7YsCNIKSA"
      },
      "outputs": [],
      "source": [
        "# preprocessing function\n",
        "def map_image(image, label):\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = image / 255.0\n",
        "\n",
        "  return image, image # dataset label is not used. replaced with the same image input.\n",
        "\n",
        "# parameters\n",
        "BATCH_SIZE = 128\n",
        "SHUFFLE_BUFFER_SIZE = 1024\n",
        "\n",
        "\n",
        "### START CODE HERE (Replace instances of `None` with your code) ###\n",
        "\n",
        "# use tfds.load() to fetch the 'train' split of CIFAR-10\n",
        "train_dataset = tfds.load(\"cifar10\", as_supervised=True, split=\"train\")\n",
        "\n",
        "# preprocess the dataset with the `map_image()` function above\n",
        "train_dataset = train_dataset.map(map_image)\n",
        "\n",
        "# shuffle and batch the dataset\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat(30)\n",
        "\n",
        "\n",
        "# use tfds.load() to fetch the 'test' split of CIFAR-10\n",
        "test_dataset = tfds.load(\"cifar10\", as_supervised=True, split=\"test\")\n",
        "\n",
        "# preprocess the dataset with the `map_image()` function above\n",
        "test_dataset = test_dataset.map(map_image)\n",
        "\n",
        "# batch the dataset\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).repeat(30)\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyOgGJs_t98"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "Create the autoencoder model. As shown in the lectures, you will want to downsample the image in the encoder layers then upsample it in the decoder path. Note that the output layer should be the same dimensions as the original image. Your input images will have the shape `(32, 32, 3)`. If you deviate from this, your model may not be recognized by the grader and may fail.\n",
        "\n",
        "We included a few hints to use the Sequential API below but feel free to remove it and use the Functional API just like in the ungraded labs if you're more comfortable with it. Another reason to use the latter is if you want to visualize the encoder output. As shown in the ungraded labs, it will be easier to indicate multiple outputs with the Functional API. That is not required for this assignment though so you can just stack layers sequentially if you want a simpler solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Wr-Bok3lRgA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a29c2c-4067-431c-ea85-48ff0f2c98bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_184 (Conv2D)         (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " max_pooling2d_79 (MaxPooli  (None, 16, 16, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_185 (Conv2D)         (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_80 (MaxPooli  (None, 8, 8, 64)          0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_186 (Conv2D)         (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_81 (MaxPooli  (None, 4, 4, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_187 (Conv2D)         (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " max_pooling2d_82 (MaxPooli  (None, 2, 2, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_188 (Conv2D)         (None, 2, 2, 256)         295168    \n",
            "                                                                 \n",
            " conv2d_189 (Conv2D)         (None, 2, 2, 128)         295040    \n",
            "                                                                 \n",
            " up_sampling2d_74 (UpSampli  (None, 4, 4, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_190 (Conv2D)         (None, 4, 4, 128)         147584    \n",
            "                                                                 \n",
            " up_sampling2d_75 (UpSampli  (None, 8, 8, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_191 (Conv2D)         (None, 8, 8, 64)          73792     \n",
            "                                                                 \n",
            " up_sampling2d_76 (UpSampli  (None, 16, 16, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_192 (Conv2D)         (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " up_sampling2d_77 (UpSampli  (None, 32, 32, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_193 (Conv2D)         (None, 32, 32, 3)         1731      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1110403 (4.24 MB)\n",
            "Trainable params: 1110403 (4.24 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# suggested layers to use. feel free to add or remove as you see fit.\n",
        "from keras.layers import Conv2D, UpSampling2D\n",
        "\n",
        "# use the Sequential API (you can remove if you want to use the Functional API)\n",
        "model = Sequential()\n",
        "\n",
        "### START CODE HERE ###\n",
        "# use `model.add()` to add layers (if using the Sequential API)\n",
        "model.add(tf.keras.layers.Input(shape=(32, 32, 3)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.UpSampling2D(size=(2,2)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.UpSampling2D(size=(2,2)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.UpSampling2D(size=(2,2)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "model.add(tf.keras.layers.UpSampling2D(size=(2,2)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=3, kernel_size=(3,3), activation='sigmoid', padding='same'))\n",
        "### END CODE HERE ###\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRWTAijKEVUC"
      },
      "source": [
        "## Configure training parameters\n",
        "\n",
        "We have already provided the optimizer, metrics, and loss in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "iHIeD9eDETSk"
      },
      "outputs": [],
      "source": [
        "# Please do not change the model.compile() parameters\n",
        "model.compile(optimizer='adam', metrics=['accuracy'], loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLQPhm1W_8dC"
      },
      "source": [
        "## Training\n",
        "\n",
        "You can now use [model.fit()](https://keras.io/api/models/model_training_apis/#fit-method) to train your model. You will pass in the `train_dataset` and you are free to configure the other parameters. As with any training, you should see the loss generally going down and the accuracy going up with each epoch. If not, please revisit the previous sections to find possible bugs.\n",
        "\n",
        "*Note: If you get a `dataset length is infinite` error. Please check how you defined `train_dataset`. You might have included a [method that repeats the dataset indefinitely](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "AMBimOnsRvg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7738a121-3601-4671-b705-a892a0116b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "91/91 [==============================] - 24s 228ms/step - loss: 0.0414 - accuracy: 0.4550 - val_loss: 0.0287 - val_accuracy: 0.4790\n",
            "Epoch 2/40\n",
            "91/91 [==============================] - 23s 255ms/step - loss: 0.0248 - accuracy: 0.5143 - val_loss: 0.0217 - val_accuracy: 0.5996\n",
            "Epoch 3/40\n",
            "91/91 [==============================] - 18s 201ms/step - loss: 0.0205 - accuracy: 0.6074 - val_loss: 0.0184 - val_accuracy: 0.6230\n",
            "Epoch 4/40\n",
            "91/91 [==============================] - 24s 265ms/step - loss: 0.0181 - accuracy: 0.6215 - val_loss: 0.0177 - val_accuracy: 0.6190\n",
            "Epoch 5/40\n",
            "91/91 [==============================] - 18s 202ms/step - loss: 0.0169 - accuracy: 0.6249 - val_loss: 0.0204 - val_accuracy: 0.6333\n",
            "Epoch 6/40\n",
            "91/91 [==============================] - 17s 194ms/step - loss: 0.0161 - accuracy: 0.6254 - val_loss: 0.0154 - val_accuracy: 0.6253\n",
            "Epoch 7/40\n",
            "91/91 [==============================] - 18s 202ms/step - loss: 0.0151 - accuracy: 0.6386 - val_loss: 0.0146 - val_accuracy: 0.6211\n",
            "Epoch 8/40\n",
            "91/91 [==============================] - 18s 194ms/step - loss: 0.0148 - accuracy: 0.6434 - val_loss: 0.0139 - val_accuracy: 0.6585\n",
            "Epoch 9/40\n",
            "91/91 [==============================] - 19s 210ms/step - loss: 0.0137 - accuracy: 0.6680 - val_loss: 0.0133 - val_accuracy: 0.6693\n",
            "Epoch 10/40\n",
            "91/91 [==============================] - 18s 198ms/step - loss: 0.0136 - accuracy: 0.6762 - val_loss: 0.0129 - val_accuracy: 0.6841\n",
            "Epoch 11/40\n",
            "91/91 [==============================] - 19s 208ms/step - loss: 0.0130 - accuracy: 0.6927 - val_loss: 0.0128 - val_accuracy: 0.6456\n",
            "Epoch 12/40\n",
            "91/91 [==============================] - 22s 248ms/step - loss: 0.0124 - accuracy: 0.6978 - val_loss: 0.0126 - val_accuracy: 0.7095\n",
            "Epoch 13/40\n",
            "91/91 [==============================] - 18s 198ms/step - loss: 0.0122 - accuracy: 0.7035 - val_loss: 0.0119 - val_accuracy: 0.7240\n",
            "Epoch 14/40\n",
            "91/91 [==============================] - 18s 196ms/step - loss: 0.0121 - accuracy: 0.6983 - val_loss: 0.0119 - val_accuracy: 0.7209\n",
            "Epoch 15/40\n",
            "91/91 [==============================] - 18s 201ms/step - loss: 0.0114 - accuracy: 0.7175 - val_loss: 0.0113 - val_accuracy: 0.7229\n",
            "Epoch 16/40\n",
            "91/91 [==============================] - 17s 192ms/step - loss: 0.0118 - accuracy: 0.7077 - val_loss: 0.0114 - val_accuracy: 0.7209\n",
            "Epoch 17/40\n",
            "91/91 [==============================] - 19s 209ms/step - loss: 0.0113 - accuracy: 0.7139 - val_loss: 0.0110 - val_accuracy: 0.7238\n",
            "Epoch 18/40\n",
            "91/91 [==============================] - 18s 196ms/step - loss: 0.0111 - accuracy: 0.7183 - val_loss: 0.0109 - val_accuracy: 0.7154\n",
            "Epoch 19/40\n",
            "91/91 [==============================] - 18s 205ms/step - loss: 0.0109 - accuracy: 0.7191 - val_loss: 0.0108 - val_accuracy: 0.7341\n",
            "Epoch 20/40\n",
            "91/91 [==============================] - 17s 194ms/step - loss: 0.0107 - accuracy: 0.7249 - val_loss: 0.0149 - val_accuracy: 0.6418\n",
            "Epoch 21/40\n",
            "91/91 [==============================] - 18s 201ms/step - loss: 0.0110 - accuracy: 0.7043 - val_loss: 0.0104 - val_accuracy: 0.7360\n",
            "Epoch 22/40\n",
            "91/91 [==============================] - 17s 193ms/step - loss: 0.0103 - accuracy: 0.7252 - val_loss: 0.0106 - val_accuracy: 0.7253\n",
            "Epoch 23/40\n",
            "91/91 [==============================] - 22s 248ms/step - loss: 0.0105 - accuracy: 0.7237 - val_loss: 0.0101 - val_accuracy: 0.7385\n",
            "Epoch 24/40\n",
            "91/91 [==============================] - 17s 194ms/step - loss: 0.0100 - accuracy: 0.7309 - val_loss: 0.0099 - val_accuracy: 0.7346\n",
            "Epoch 25/40\n",
            "91/91 [==============================] - 18s 199ms/step - loss: 0.0104 - accuracy: 0.7223 - val_loss: 0.0104 - val_accuracy: 0.7397\n",
            "Epoch 26/40\n",
            "91/91 [==============================] - 17s 194ms/step - loss: 0.0098 - accuracy: 0.7324 - val_loss: 0.0097 - val_accuracy: 0.7447\n",
            "Epoch 27/40\n",
            "91/91 [==============================] - 18s 198ms/step - loss: 0.0100 - accuracy: 0.7257 - val_loss: 0.0096 - val_accuracy: 0.7349\n",
            "Epoch 28/40\n",
            "91/91 [==============================] - 17s 192ms/step - loss: 0.0096 - accuracy: 0.7335 - val_loss: 0.0094 - val_accuracy: 0.7441\n",
            "Epoch 29/40\n",
            "91/91 [==============================] - 18s 201ms/step - loss: 0.0101 - accuracy: 0.7206 - val_loss: 0.0099 - val_accuracy: 0.7404\n",
            "Epoch 30/40\n",
            "91/91 [==============================] - 17s 190ms/step - loss: 0.0095 - accuracy: 0.7331 - val_loss: 0.0093 - val_accuracy: 0.7431\n",
            "Epoch 31/40\n",
            "91/91 [==============================] - 18s 199ms/step - loss: 0.0097 - accuracy: 0.7293 - val_loss: 0.0095 - val_accuracy: 0.7380\n",
            "Epoch 32/40\n",
            "91/91 [==============================] - 17s 191ms/step - loss: 0.0092 - accuracy: 0.7365 - val_loss: 0.0094 - val_accuracy: 0.7248\n",
            "Epoch 33/40\n",
            "91/91 [==============================] - 17s 190ms/step - loss: 0.0092 - accuracy: 0.7385 - val_loss: 0.0092 - val_accuracy: 0.7192\n",
            "Epoch 34/40\n",
            "91/91 [==============================] - 18s 197ms/step - loss: 0.0093 - accuracy: 0.7329 - val_loss: 0.0090 - val_accuracy: 0.7441\n",
            "Epoch 35/40\n",
            "91/91 [==============================] - 17s 190ms/step - loss: 0.0091 - accuracy: 0.7346 - val_loss: 0.0091 - val_accuracy: 0.7435\n",
            "Epoch 36/40\n",
            "91/91 [==============================] - 18s 197ms/step - loss: 0.0091 - accuracy: 0.7369 - val_loss: 0.0094 - val_accuracy: 0.7495\n",
            "Epoch 37/40\n",
            "91/91 [==============================] - 17s 191ms/step - loss: 0.0089 - accuracy: 0.7439 - val_loss: 0.0094 - val_accuracy: 0.7295\n",
            "Epoch 38/40\n",
            "91/91 [==============================] - 18s 196ms/step - loss: 0.0091 - accuracy: 0.7369 - val_loss: 0.0088 - val_accuracy: 0.7401\n",
            "Epoch 39/40\n",
            "91/91 [==============================] - 17s 190ms/step - loss: 0.0089 - accuracy: 0.7386 - val_loss: 0.0087 - val_accuracy: 0.7527\n",
            "Epoch 40/40\n",
            "91/91 [==============================] - 17s 194ms/step - loss: 0.0087 - accuracy: 0.7434 - val_loss: 0.0087 - val_accuracy: 0.7504\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ec1e21dd060>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "# parameters (feel free to change this)\n",
        "train_steps = len(train_dataset) // BATCH_SIZE\n",
        "val_steps = len(test_dataset) // BATCH_SIZE\n",
        "\n",
        "### START CODE HERE ###\n",
        "model.fit(train_dataset, steps_per_epoch=train_steps, validation_data=test_dataset, epochs=40)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT2l1c-SAaF4"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "You can use this code to test your model locally before uploading to the grader. To pass, your model needs to satisfy these two requirements:\n",
        "\n",
        "* loss must be less than 0.01\n",
        "* accuracy must be greater than 0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "vFncgqahSQhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd389f5-e023-4500-bdaf-fcbc10756f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0086 - accuracy: 0.7521\n"
          ]
        }
      ],
      "source": [
        "result = model.evaluate(test_dataset, steps=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di6VOHGwIsVM"
      },
      "source": [
        "If you did some visualization like in the ungraded labs, then you might see something like the gallery below. This part is not required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmpI4skkIA5L"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/tensorflow-3-public/assets/images/autoencoder.png\" width=\"75%\" height=\"75%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvw0HLY2kV3w"
      },
      "source": [
        "## Save the Model\n",
        "\n",
        "Once you're satisfied with the results, you can save your model and upload it to the grader in Coursera. Please run all succeeding cells to ensure that you will have a gradable submission. Otherwise, you might get this error message:\n",
        "\n",
        "`There was a problem grading your submission. Check stderr for more details.`\n",
        "\n",
        "First, save the model file in your Colab workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ULCfGHEKkaO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c35db49-5636-481f-8e22-b31d5bd9fda7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the model you just trained\n",
        "model.save(\"temp_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuUoVmXBZYqc"
      },
      "source": [
        "Tensorflow releases minor and patch updates every few months ([sometimes quicker](https://pypi.org/project/tensorflow/#history)). Oftentimes, the code and syntax won't change but the newer versions will have a different model definition under the hood. The next cells will convert your newer model to one that the grader can read. You won't have to do this in your personal projects. This is just an extra step for grading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "OxpkHjH40s_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdcf3dbb-5baf-48ed-a23d-791c15574ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install packages for compatibility with the autograder\n",
        "\n",
        "# NOTE: You can safely ignore errors about version incompatibility of\n",
        "# Colab-bundled packages (e.g. xarray, pydantic, etc.)\n",
        "\n",
        "!pip install tensorflow==2.8.0 --quiet\n",
        "!pip install keras==2.8.0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQf0BVmsXIAM"
      },
      "source": [
        "Wait for the cell above to complete. After that, **go to `Runtime > Restart Session` and wait for the kernel to reconnect.** This is necessary so you can reload Tensorflow with the version installed above. Run the next cell to check if you have the correct version. You will get an `AssertionError` if you didn't restart the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TEcDAyaNMyW-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if you have the correct Tensorflow version\n",
        "assert tf.__version__ == '2.8.0', f'You have TF{tf.__version__}. Please install the grader-compatible Tensorflow and select Runtime > Restart Session'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaFHTqWufL9F"
      },
      "source": [
        "If you didn't get an error above, you can finally convert the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ljeWKuSKYEHE"
      },
      "outputs": [],
      "source": [
        "# Load the model you saved earlier\n",
        "model = tf.keras.models.load_model(\"temp_model.h5\", compile=False)\n",
        "\n",
        "# For this assignment only. The model has to be compiled with these settings.\n",
        "model.compile(optimizer='adam', metrics=['accuracy'], loss='mean_squared_error')\n",
        "\n",
        "# Save the model with the compatible TF version\n",
        "model.save(\"mymodel.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG4JE2JNYGUG"
      },
      "source": [
        "Please look for `mymodel.h5` in the File Explorer on the left and download it. Then go back to the Coursera classroom and upload it to `My Submissions` tab of the Week 2 assignment. Alternatively, you can also use the cell below to download the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NCd50-pubX_o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "139870c7-c4d5-4860-d507-b4fedb42f59a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c7c4b85a-80f7-48fc-bed8-730540107849\", \"mymodel.h5\", 4495456)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# You can also use this cell as a shortcut for downloading your model\n",
        "from google.colab import files\n",
        "files.download(\"mymodel.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QArMiXJTDxDe"
      },
      "source": [
        "**Congratulations on completing this week's assignment!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}